{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id='project-id', project_access_token='project-token')\n",
    "pc = project.project_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ DATA FROM DB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ibmdbpy import IdaDataBase, IdaDataFrame\n",
    "import string\n",
    "import emoji\n",
    "from collections import Counter,defaultdict,OrderedDict\n",
    "from tweebo_parser import API, ServerError\n",
    "import pyodbc\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = 'credential-db'\n",
    "db = IdaDataBase(dsn=conn_string)\n",
    "db.show_tables(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = IdaDataFrame(db, 'TGZ44984.USER').as_dataframe()\n",
    "biz = IdaDataFrame(db, 'TGZ44984.BUSINESS').as_dataframe()\n",
    "# Big file with varchar more than 1024 can't use IdaDataFrame, use pyodbc\n",
    "cnx = pyodbc.connect('DSN='+conn_string)\n",
    "cursor = cnx.cursor()\n",
    "cursor.execute('select * from TGZ44984.REVIEW')\n",
    "rows = cursor.fetchall()\n",
    "rows = [tuple(x) for x in rows]\n",
    "review = pd.DataFrame(rows,columns = ['review_id','user_id','biz_id','stars','useful','funny','cool','text','date'])\n",
    "cursor.close()\n",
    "cnx.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN NLU API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "# import watson_developer_cloud.natural_language_understanding.features.v1 as Features\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, KeywordsOptions\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator('IAMAuth')\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2019-07-12',\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "natural_language_understanding.set_service_url('https://gateway.watsonplatform.net/natural-language-understanding/api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in review.iterrows():\n",
    "    id_  = row['review_id']\n",
    "    text_ = row['text']\n",
    "    if re.search(r'\\w+',text_) is None or len(text_) < 10:\n",
    "        continue\n",
    "    response = natural_language_understanding.analyze(text=text_,\n",
    "                                                      features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True))).get_result()\n",
    "    re_ = response['keywords']\n",
    "    lang_ = response['language']\n",
    "    result.append({'review_id':id_, 'result':re_, 'lang':lang_})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data(\"result.json\", json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEYWORDS TO ASPECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b300d = nlp.embedding.create('glove', source='glove.6B.300d')\n",
    "vocab = nlp.Vocab(nlp.data.Counter(glove_6b300d.idx_to_token))\n",
    "vocab.set_embedding(glove_6b300d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "transtab = str.maketrans(punct,len(punct)*\" \")\n",
    "aspects = ['cleanliness','food','service','location','price']\n",
    "emo_list = ['sadness','joy','fear','disgust','anger']\n",
    "cols = ['review_id']+[i+'_sentiment' for i in aspects]+[i+'_'+j for i in aspects for j in emo_list ]\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "final_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(x, y):\n",
    "    return nd.dot(x, y) / (nd.norm(x) * nd.norm(y))\n",
    "\n",
    "def check_similarity(word, aspects = aspects):\n",
    "    similarity = []\n",
    "    for aspect in aspects:\n",
    "        similarity.append(cos_sim(vocab.embedding[aspect],vocab.embedding[word]).asnumpy()[0])\n",
    "    \n",
    "    if max(similarity) > 0.30:\n",
    "        return aspects[np.argmax(similarity)]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_30d96c6e27154b7d92410a8469740ae4 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='apikey',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_30d96c6e27154b7d92410a8469740ae4.get_object(Bucket='bucket',Key='result.json')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object \n",
    "\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face an error during data loading.\n",
    "# Please read the documentation of 'pandas.read_json()' and 'pandas.io.json.json_normalize' to learn more about the possibilities to adjust the data loading.\n",
    "# pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader\n",
    "# and http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "\n",
    "# df_data_1 = pd.read_json(body, orient='values')\n",
    "# df_data_1.head()\n",
    "\n",
    "re_list = json.load(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in re_list:\n",
    "    row = dict(zip(cols,[None for i in range(len(cols))]))\n",
    "    row['review_id'] = res['review_id']\n",
    "    for sent_ in res['result']:\n",
    "        doc = nlp(sent_['text'])\n",
    "        pos_tag = [token.text for token in doc if token.pos_ in ['NOUN', 'PRON', 'PROPN']]\n",
    "        text_ = [re.sub('\\s+','',x.translate(transtab).lower()) for x in pos_tag]\n",
    "        \n",
    "        asp_ = None\n",
    "        for t_ in text_:\n",
    "            asp_ = check_similarity(aspects,t_)\n",
    "            \n",
    "            if asp_ is not None:\n",
    "                break\n",
    "        if asp_ is None:\n",
    "            continue\n",
    "        if row[asp_+'_sentiment'] is None:\n",
    "            row[asp_+'_sentiment'] = []\n",
    "        score_ = sent_['sentiment']['score']\n",
    "        label_ = sent_['sentiment']['label']\n",
    "        if (label_ == 'negative' and score_ > 0) or (label_ == 'positive' and score_ < 0):\n",
    "            score = -score\n",
    "        row[asp_+'_sentiment'].append(score_)\n",
    "        if 'emotion' in sent_.keys():\n",
    "            emo_ = Counter(sent_['emotion']).most_common(1)[0]\n",
    "            emo_label = asp_+'_'+emo_[0]\n",
    "            if row[emo_label] is None:\n",
    "                row[emo_label] = []\n",
    "            row[emo_label].append(emo_[1])\n",
    "    for col in cols[1:]:\n",
    "        if isinstance(row[col],list):\n",
    "            row[col] = np.mean(row[col])\n",
    "    final_result.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data('result.csv', pd.DataFrame(final_result).to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
