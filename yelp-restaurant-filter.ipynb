{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter,defaultdict,OrderedDict\n",
    "from tweebo_parser import API, ServerError\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./yelp_dataset/business.json','r',encoding='utf-8') as f:\n",
    "    biz_list = list(map(json.loads, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_food = set(['OutdoorSeating','Alcohol','RestaurantsGoodForGroups','RestaurantsAttire',\n",
    "            'RestaurantsReservations','RestaurantsTakeOut','RestaurantsDelivery','Caters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "transtab = str.maketrans(punct,len(punct)*\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_food = ['food','cafe','restaurant','bar','dine','drink','bistro','pub','eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_food(entry):\n",
    "    if 'attributes' in entry.keys() and entry['attributes'] is not None:\n",
    "        attr_ = set(entry['attributes'].keys())\n",
    "        if len(attr_.intersection(attr_food))>0:\n",
    "            return True\n",
    "#     else: \n",
    "#         if 'categories' in entry.keys() and entry['categories'] is not None:\n",
    "#             cat_ = re.sub('\\s+',' ', str(entry['categories']).translate(transtab)).lower().split(' ')\n",
    "#             sim_ = np.array([cos_sim(vocab.embedding[c], vocab.embedding[key]) for c in cat_ for key in keyword_food])\n",
    "#             if len(np.where(sim_>0.3)[0])>0:\n",
    "#                 return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_biz = [(ent_['business_id'],ent_['name'],ent_['address'],ent_['city'],ent_['state'],ent_['postal_code'],\n",
    "              ent_['latitude'],ent_['longitude'],ent_['stars'],ent_['review_count'],\n",
    "              ent_['is_open']) for ent_ in biz_list if filter_food(ent_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz = pd.DataFrame(filter_biz, columns = ['business_id','name','address','city','state','postal_code','latitide','longitude',\n",
    "                                         'stars','review_count','is_open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./yelp_dataset/review.json','r',encoding='utf-8') as f:\n",
    "    review_list = list(map(json.loads, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_id = biz.loc[biz.state=='PA','business_id'].values #penn state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_review,i = [],0\n",
    "\n",
    "for rev in review_list:\n",
    "    if i >100000: # restrict to 100K to fit into 200mb\n",
    "        break\n",
    "    else:\n",
    "        if rev['business_id'] in biz_id:\n",
    "            filter_review.append(rev)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.DataFrame(filter_review)\n",
    "biz_high = pd.DataFrame(review.groupby('business_id')['date'].count()>100).reset_index()\n",
    "biz_h_list = biz_high.loc[biz_high.date==True,'business_id'].values\n",
    "review = review.loc[review.business_id.isin(biz_h_list)]\n",
    "del biz_list, review_list, filter_biz, filter_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.stars.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.groupby('stars')['business_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review.to_csv('./review_selected.csv',index=False)\n",
    "# export to keep a copy\n",
    "# left with 53920 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv('./review_selected.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review.user_id.unique()) \n",
    "# left with 2K+ unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review.business_id.unique())\n",
    "# left with 200+ restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZE, PARSE & TEST WITH TDSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from bella import helper\n",
    "from bella.models.target import TargetDep\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweebo_api = API(hostname='localhost',port=8000)\n",
    "text_data = [re.sub('\\s+',' ',review.text.iloc[0]),\n",
    "             re.sub('\\s+',' ',review.text.iloc[-1])]\n",
    "try:\n",
    "    result_conll = tweebo_api.parse_conll(text_data)\n",
    "except ServerError as e:\n",
    "    print(f'{e}\\n{e.message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_root_node(list_conll_sentences):\n",
    "    temp_list_conll_sentences = []\n",
    "    for conll_sentences in list_conll_sentences:\n",
    "        temp_conll_sentences = []\n",
    "        for sentence in conll_sentences.split('\\n'):\n",
    "            sentence = sentence.split('\\t')\n",
    "            if int(sentence[6]) == 0:\n",
    "                sentence[7] = 'ROOT'\n",
    "            temp_conll_sentences.append('\\t'.join(sentence))\n",
    "        conll_sentences = '\\n'.join(temp_conll_sentences)\n",
    "        temp_list_conll_sentences.append(conll_sentences)\n",
    "    return temp_list_conll_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_result = add_root_node(result_conll)\n",
    "nltk_dep_tree = DependencyGraph(nltk_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_dep_tree.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_dep_tree.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dep = helper.download_model(TargetDep, 'SemEval 14 Restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "parser_col = ['ID','TOKEN','LEMMA','UPOS','XPOS','FEATS','HEAD','DEPREL','DEPS','MISC']\n",
    "def extract_entities():\n",
    "    ent_series = []\n",
    "    for res in result_conll:\n",
    "        sub_ = pd.DataFrame([token.split('\\t') for token in res.split('\\n')],columns=parser_col)\n",
    "        entities = [tuple([int(i),x]) for i,x in sub_.loc[sub_.UPOS.isin(['^','N']),['ID','TOKEN']].values]\n",
    "        ent_lst = []\n",
    "        if len(entities) > 0:\n",
    "            start, end, ent_ = entities[0][0],entities[0][0],entities[0][1]\n",
    "            if len(entities) == 1:\n",
    "                if 'UNK' not in ent_:\n",
    "                    ent_lst.append((ent_,start,end))\n",
    "            else:\n",
    "                for i, token in entities[1:]:\n",
    "                    if end == i-1 and 'UNK' not in \" \".join([ent_,token]):\n",
    "                        ent_ = \" \".join([ent_,token])\n",
    "                        end = i\n",
    "                        if (i, token) == entities[-1]:\n",
    "                            ent_lst.append((ent_,start,end))\n",
    "                    else:\n",
    "                        if 'UNK' not in ent_:\n",
    "                            ent_lst.append((ent_,start,end))\n",
    "                            start, end = i,i\n",
    "                            ent_ = token\n",
    "                            if (i, token) == entities[-1]:\n",
    "                                ent_lst.append((ent_,start,end))\n",
    "                                \n",
    "        ent_series.append((ent_lst,len(ent_lst)))\n",
    "    return ent_series\n",
    "temp = extract_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(2):\n",
    "    txt_ = text_data[x]\n",
    "    ent_lst = [ent_ for ent_, start, end in temp[x][0]]\n",
    "    spans = [re.search(e_, txt_).span() for e_ in ent_lst]\n",
    "    for j in range(len(ent_lst)):\n",
    "        d_ = {'text':txt_, 'target':ent_lst[j],'spans':spans[j]}\n",
    "        test_data.append(d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dep.probabilities(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
